{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 550-01-Dtree\n",
    "## Team members\n",
    "### 1) Sean Pereira - sean.pereira@student.csulb.edu\n",
    "### 2) Sushmitha Pasala - sushmitha.pasala@student.csulb.edu\n",
    "### 3) Vatsal Patel - vatsal.patel01@student.csulb.edu\n",
    "##### This file creates two decision tree and checks its accuracy on selected holdout set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self,depth=0,max_depth=8):\n",
    "        #Read the data from csv file and name the columns\n",
    "        \n",
    "        c=['White King file (column)','White King rank (row)','White Rook file','White Rook rank','Black King file','Black King rank','Output']\n",
    "        self.df=pd.read_csv('550-p1-cset-krk-1.csv',header=None)\n",
    "        self.df=self.df.rename({0:'White King file (column)',1:'White King rank (row)',2:'White Rook file',3:'White Rook rank',4:'Black King file',5:'Black King rank',6:'Output'}, axis=1)\n",
    "        df0,df1,df2,df3,df4,df5=self.processing_data(self.df)\n",
    "        self.label_output()\n",
    "        self.df=pd.concat([df0,df1,df2,df3,df4,df5,self.df['Output']],axis=1)\n",
    "        self.left=None\n",
    "        self.right=None\n",
    "        self.fkey=None\n",
    "        self.fval=None\n",
    "        self.depth=depth\n",
    "        self.max_depth=max_depth\n",
    "        self.target=None\n",
    "        self.d1={17:'draw',0:'zero',1:'one',2:'two',3:'three',4:'four',5:'five',6:'six',7:'seven',8:'eight',9:'nine',10:'ten',11:'eleven',12:'twelve',13:'thirteen',14:'fourteen',15:'fifteen',16:'sixteen'}\n",
    "\n",
    "    def label_output(self):\n",
    "        self.d={'draw':17,'zero':0,'one':1,'two':2,'three':3,'four':4,'five':5,'six':6,'seven':7,'eight':8,'nine':9,'ten':10,'eleven':11\n",
    "          ,'twelve':12,'thirteen':13,'fourteen':14,'fifteen':15,'sixteen':16}\n",
    "        for column in self.df:\n",
    "            if column=='Output':\n",
    "                s1=self.df[column].values\n",
    "                for j,i in enumerate(s1):\n",
    "                    s1[j]=self.d[i]\n",
    "                break\n",
    "        self.df=self.df.assign(Output=s1,inplace='True')\n",
    "\n",
    "        \n",
    "        \n",
    "    def processing_data(self,data):\n",
    "        # Labeling each data to 0-1, converting categorical to numerical data\n",
    "        \n",
    "        columns_text_0=['WKa','WKb','WKc','WKd','WKe','WKf','WKg','WKh']\n",
    "        columns_data_0=['WK1','WK2','WK3','WK4','WK5','WK6','WK7','WK8']\n",
    "        columns_text_1=['WRa','WRb','WRc','WRd','WRe','WRf','WRg','WRh']\n",
    "        columns_data_1=['WR1','WR2','WR3','WR4','WR5','WR6','WR7','WR8']\n",
    "        columns_text_2=['BKa','BKb','BKc','BKd','BKe','BKf','BKg','BKh']\n",
    "        columns_data_2=['BK1','BK2','BK3','BK4','BK5','BK6','BK7','BK8']\n",
    "        index=0\n",
    "        for i in ['White King file (column)','White King rank (row)','White Rook file','White Rook rank','Black King file','Black King rank']:\n",
    "            alphabets=[]\n",
    "            numericals=[]\n",
    "            for columndata in data[i]:\n",
    "                letter=[0]*8\n",
    "                numbers=[0]*8\n",
    "                if not isinstance(columndata, int):\n",
    "                    letter[ord(columndata)-ord('a')]=1\n",
    "                    alphabets.append(letter)\n",
    "                else:\n",
    "                    numbers[ord(str(columndata))-ord('0')-1]=1\n",
    "                    numericals.append(numbers)\n",
    "            if index==0:\n",
    "                df0=pd.DataFrame(data=alphabets, columns=columns_text_0)\n",
    "            if index==1:\n",
    "                df1=pd.DataFrame(data=numericals, columns=columns_data_0)\n",
    "            if index==2:\n",
    "                df2=pd.DataFrame(data=alphabets, columns=columns_text_1)\n",
    "            if index==3:\n",
    "                df3=pd.DataFrame(data=numericals, columns=columns_data_1)\n",
    "            if index==4:\n",
    "                df4=pd.DataFrame(data=alphabets, columns=columns_text_2)\n",
    "            if index==5:\n",
    "                df5=pd.DataFrame(data=numericals, columns=columns_data_2)\n",
    "            index+=1\n",
    "        return (df0,df1,df2,df3,df4,df5)\n",
    "    \n",
    "    def entropy(self,col):\n",
    "        counts=np.unique(col,return_counts=True)\n",
    "        ent=0.0\n",
    "        for i in counts[1]:\n",
    "            p=i/col.shape[0]\n",
    "            ent+=(-1.0*p*np.log2(p))\n",
    "        return ent\n",
    "    \n",
    "    def information_gain(self,x_data,fkey,fval):\n",
    "        right,left=self.divide_data(x_data,fkey,fval)\n",
    "        l=float(left.shape[0])/x_data.shape[0]\n",
    "        r=float(right.shape[0])/x_data.shape[0]\n",
    "        if left.shape[0]==0 or right.shape[0]==0:\n",
    "            return float(\"-inf\")\n",
    "        i_gain=self.entropy(x_data.Output)-(l*self.entropy(left.Output)+r*self.entropy(right.Output))\n",
    "        return i_gain\n",
    "    \n",
    "    def divide_data(self,x_data,fkey,fval):\n",
    "        \n",
    "        #fkey: Feature names \n",
    "        #fval: \n",
    "        \n",
    "        x_right=pd.DataFrame([],columns=x_data.columns)\n",
    "        x_left=pd.DataFrame([],columns=x_data.columns)\n",
    "        for i in range(x_data.shape[0]):\n",
    "            val = x_data[fkey].loc[i]\n",
    "            if val >= fval:\n",
    "                x_right = x_right.append(x_data.iloc[i])\n",
    "            else:\n",
    "                x_left = x_left.append(x_data.iloc[i])\n",
    "        return x_right,x_left\n",
    "    \n",
    "    def frequency_of_Output(self, x_train):\n",
    "        \n",
    "        self.dict={}\n",
    "        for i in x_train:\n",
    "            if i not in self.dict:\n",
    "                self.dict[i]=1\n",
    "            else:\n",
    "                self.dict[i]+=1\n",
    "        return max(self.dict, key= lambda d: self.dict[d])\n",
    "        \n",
    "    def train(self,x_train):\n",
    "        features=self.df.columns[:-1]\n",
    "        info_gains=[]\n",
    "        for i in features:\n",
    "            i_gain=self.information_gain(x_train,i,0.5)\n",
    "            info_gains.append(i_gain)\n",
    "        self.fkey=features[np.argmax(info_gains)]\n",
    "        self.fval=0.5\n",
    "        #print(\"Splitting Tree\",self.fkey,\"entropy\",max(info_gains))\n",
    "        data_right,data_left=self.divide_data(x_train,self.fkey,self.fval)\n",
    "        data_right=data_right.reset_index(drop=True)\n",
    "        data_left=data_left.reset_index(drop=True)\n",
    "        if data_left.shape[0]==0 or data_right.shape[0]==0:\n",
    "            self.target=self.d1[self.frequency_of_Output(x_train.Output)]\n",
    "            return \n",
    "        if self.depth>=self.max_depth:\n",
    "            \n",
    "            self.target=self.d1[self.frequency_of_Output(x_train.Output)]\n",
    "            return \n",
    "        self.left=DecisionTree(self.depth+1,self.max_depth)\n",
    "        self.left.train(data_left)\n",
    "        self.right=DecisionTree(self.depth+1,self.max_depth)\n",
    "        self.right.train(data_right)\n",
    "\n",
    "        self.target=self.d1[self.frequency_of_Output(x_train.Output)]\n",
    "        return \n",
    "    \n",
    "    def predict(self,test):\n",
    "        if test[self.fkey] > self.fval:\n",
    "            if self.right is None:\n",
    "                return self.target\n",
    "            return self.right.predict(test)\n",
    "        if test[self.fkey] <= self.fval:\n",
    "            if self.left is None:\n",
    "                return self.target\n",
    "            return self.left.predict(test)\n",
    "    def dataframe(self):\n",
    "        return self.df\n",
    "\n",
    "    \n",
    "\n",
    "        \n",
    "#Creating Object of Decision Tree\n",
    "d=DecisionTree()\n",
    "\n",
    "\n",
    "\n",
    "# Splitting Data Into training, test and validate :60,20,20\n",
    "train_data, validate_data, test_data = np.split(d.dataframe().sample(frac=1,random_state=42), [int(.6*len(d.dataframe())), int(.8*len(d.dataframe()))])\n",
    "\n",
    "#Reset Index to 0\n",
    "train_data=train_data.reset_index(drop=True)\n",
    "test_data=test_data.reset_index(drop=True)\n",
    "\n",
    "# Building tree\n",
    "d.train(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bagging_replacement(t_set, holdout_set,d):\n",
    "    final_t_set = []\n",
    "    final_holdout_set = []\n",
    "    \n",
    "    Training_indexes = list(t_set.index)\n",
    "    Testing_indexes = list(holdout_set.index)\n",
    "    union_set=Training_indexes+Testing_indexes\n",
    "    union_set.sort()\n",
    "    \n",
    "    incorrect_array=accuracy(d,holdout_set)[1]\n",
    "\n",
    "\n",
    "    for i in incorrect_array:\n",
    "        union_set.append(i)\n",
    "        union_set.append(i)\n",
    "\n",
    "    for _ in range(len(t_set)):\n",
    "        add_index = random.randint(0, len(t_set) - 1)\n",
    "        final_t_set.append(union_set[add_index])\n",
    "\n",
    "\n",
    "\n",
    "    # remove duplicates before removing items in final_t_set\n",
    "    for item in union_set:\n",
    "        if item not in final_holdout_set:\n",
    "            final_holdout_set.append(item)\n",
    "    \n",
    "    for item in final_t_set:\n",
    "        if item in final_holdout_set:\n",
    "            final_holdout_set.remove(item)\n",
    "\n",
    "\n",
    "    return final_t_set, final_holdout_set\n",
    "\n",
    "def accuracy(d,test_data):\n",
    "\n",
    "    count=0\n",
    "    incorrect=[]\n",
    "    correct=[]\n",
    "    old_data=test_data.index\n",
    "\n",
    "    test_data=test_data.reset_index(drop=True)\n",
    "    y_pred=[]\n",
    "\n",
    "    for i in range(test_data.shape[0]):\n",
    "        y_pred.append(d.predict(test_data.loc[i]))\n",
    "\n",
    "\n",
    "    for i in range(len(y_pred)):\n",
    "        if y_pred[i]== d.d1[test_data['Output'][i]]:\n",
    "            count+=1\n",
    "            correct.append(i)\n",
    "        else:\n",
    "            incorrect.append(i)\n",
    "    \n",
    "\n",
    "        \n",
    "    \n",
    "    new_data=[]\n",
    "    for i in incorrect:\n",
    "        new_data.append(old_data[i])  \n",
    "    return count/len(test_data),new_data\n",
    "\n",
    "\n",
    "#print(\"Accuracy of 1st DTree:\",accuracy(d,test_data)[0]*100,\"%\")\n",
    "train_data, validate_data, test_data = np.split(d.dataframe().sample(frac=1,random_state=42), [int(.6*len(d.dataframe())), int(.8*len(d.dataframe()))])\n",
    "Training_Set, Holdout_Set = bagging_replacement(train_data, test_data,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_indices_to_DataFrame(Training_Set,d):\n",
    "    index1=[]\n",
    "    Training_Set.sort()   \n",
    "    d1=[]\n",
    "    for i, j in d.dataframe().iterrows():\n",
    "        if i in Training_Set:\n",
    "            c1=Training_Set.count(i)\n",
    "            for _ in range(c1):\n",
    "                d1.append(d.dataframe()[i:i+1].values)\n",
    "    v1=[]\n",
    "    for i in d1:\n",
    "        b1=[]\n",
    "        for t in i:\n",
    "            for r in t:\n",
    "                b1.append(r)\n",
    "        v1.append(b1)\n",
    "    return v1\n",
    "\n",
    "\n",
    "#d1=DecisionTree()\n",
    "Training_Set_d2 = pd.DataFrame(data= convert_indices_to_DataFrame(Training_Set,d),columns=d.dataframe().columns)\n",
    "HoldOut_Set_d2 = pd.DataFrame(data=  convert_indices_to_DataFrame(Holdout_Set,d),columns=d.dataframe().columns)\n",
    "d.train(Training_Set_d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Accuracy of 2nd DTree:\",accuracy(d,HoldOut_Set_d2 )[0]*100,\"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
